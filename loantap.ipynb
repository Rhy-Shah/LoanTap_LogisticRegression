{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoanTap is an online platform committed to delivering customized loan products to millennials. They innovate in an otherwise dull loan segment, to deliver instant, flexible loans on consumer friendly terms to salaried professionals and businessmen.\n",
    "\n",
    "The data science team at LoanTap is building an underwriting layer to determine the creditworthiness of MSMEs as well as individuals.\n",
    "\n",
    "LoanTap deploys formal credit to salaried individuals and businesses 4 main financial instruments:\n",
    "\n",
    "Personal Loan\n",
    "EMI Free Loan\n",
    "Personal Overdraft\n",
    "Advance Salary Loan\n",
    "This case study will focus on the underwriting process behind Personal Loan only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data\n",
    "\n",
    "loan_amnt : The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\n",
    "\n",
    "term : The number of payments on the loan. Values are in months and can be either 36 or 60.\n",
    "\n",
    "int_rate : Interest Rate on the loan\n",
    "\n",
    "installment : The monthly payment owed by the borrower if the loan originates.\n",
    "\n",
    "grade : LoanTap assigned loan grade\n",
    "\n",
    "sub_grade : LoanTap assigned loan subgrade\n",
    "\n",
    "emp_title :The job title supplied by the Borrower when applying for the loan.*\n",
    "\n",
    "emp_length : Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\n",
    "\n",
    "home_ownership : The home ownership status provided by the borrower during registration or obtained from the credit report.\n",
    "\n",
    "annual_inc : The self-reported annual income provided by the borrower during registration.\n",
    "\n",
    "verification_status : Indicates if income was verified by LoanTap, not verified, or if the income source was verified\n",
    "\n",
    "issue_d : The month which the loan was funded\n",
    "\n",
    "loan_status : Current status of the loan - Target Variable\n",
    "\n",
    "purpose : A category provided by the borrower for the loan request.\n",
    "\n",
    "title : The loan title provided by the borrower\n",
    "\n",
    "dti : A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LoanTap loan, divided by the borrower’s self-reported monthly income.\n",
    "\n",
    "earliest_cr_line :The month the borrower's earliest reported credit line was opened\n",
    "\n",
    "open_acc : The number of open credit lines in the borrower's credit file.\n",
    "\n",
    "pub_rec : Number of derogatory public records\n",
    "\n",
    "revol_bal : Total credit revolving balance\n",
    "\n",
    "revol_util : Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n",
    "\n",
    "total_acc : The total number of credit lines currently in the borrower's credit file\n",
    "\n",
    "initial_list_status : The initial listing status of the loan. Possible values are – W, F\n",
    "\n",
    "application_type : Indicates whether the loan is an individual application or a joint application with two co-borrowers\n",
    "\n",
    "mort_acc : Number of mortgage accounts.\n",
    "\n",
    "pub_rec_bankruptcies : Number of public record bankruptcies\n",
    "\n",
    "Address: Address of the individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"logistic_regression.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print(i, '-->> ', df[i].unique(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping all null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicates in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_columns = df.select_dtypes(include=['float64']).columns\n",
    "float_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_float = df[float_columns]\n",
    "df_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df_float.corr(method='spearman'),annot=True,cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed almost perfect correlation between \"loan_amnt\" the \"installment\" feature.\n",
    "\n",
    "installment: The monthly payment owed by the borrower if the loan originates.\n",
    "loan_amnt: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\n",
    "So, we can drop either one of those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['installment'],axis=1,inplace=True)\n",
    "df_float.drop(columns=['installment'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df_float.corr(method='spearman'),annot=True,cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_cols = df.select_dtypes('float64').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in univariate_cols:\n",
    "#     plt.figure(figsize=(12,5))\n",
    "    plt.title(\"Distribution of {}\".format(i))\n",
    "    sns.histplot(df[i]/df[i].max(), kde=True, bins=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the distribution is highly skewed which tells us that they might contain outliers\n",
    "Almost all the continuous features have outliers present in the dataset. They have to be standarsised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = ['home_ownership', 'verification_status', 'loan_status', 'application_type', 'grade', 'sub_grade', 'term']\n",
    "for i in cat_vars: \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.title(f'Distribution of {i}')\n",
    "    sns.countplot(data=df, x=i)\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the application type is Individual\n",
    "\n",
    "Most of the loan tenure is disbursed for 36 months\n",
    "\n",
    "The grade of majority of people those who have took the loan is 'B' and have subgrade 'B3'.\n",
    "\n",
    "So from that we can infer that people with grade 'B' and subgrade 'B3' are more likely to fully pay the loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the people took loan for 36 months and full paid on time\n",
    "\n",
    "Most of people have home ownership as mortgage and rent\n",
    "\n",
    "Most of the people took loan for debt consolidations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the application type is Individual\n",
    "Most of the loan tenure is disbursed for 36 months\n",
    "The grade of majority of people those who have took the loan is 'B' and have subgrade 'B3'.\n",
    "So from that we can infer that people with grade 'B' and subgrade 'B3' are more likely to fully pay the loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "plt.figure(figsize=(20,30))\n",
    "for i in univariate_cols:\n",
    "    count += 1\n",
    "    plt.subplot(5,3,count)\n",
    "    sns.boxplot(y= df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in univariate_cols:\n",
    "    mean=df[col].mean()\n",
    "    std=df[col].std()\n",
    "    \n",
    "    upper_limit=mean+3*std\n",
    "    lower_limit=mean-3*std\n",
    "    \n",
    "    df=df[(df[col]<upper_limit) & (df[col]>lower_limit)]\n",
    "    \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,20))\n",
    "\n",
    "plt.subplot(4,2,1)\n",
    "sns.countplot(x='term',data=df,hue='loan_status')\n",
    "\n",
    "plt.subplot(4,2,2)\n",
    "sns.countplot(x='home_ownership',data=df,hue='loan_status')\n",
    "\n",
    "plt.subplot(4,2,3)\n",
    "sns.countplot(x='verification_status',data=df,hue='loan_status')\n",
    "\n",
    "plt.subplot(4,2,4)\n",
    "g=sns.countplot(x='purpose',data=df,hue='loan_status')\n",
    "g.set_xticklabels(g.get_xticklabels(),rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the people took loan for 36 months and full paid on time\n",
    "\n",
    "Most of people have home ownership as mortgage and rent\n",
    "\n",
    "Most of the people took loan for debt consolidations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. People with grades ‘A’ are more likely to fully pay their loan. (T/F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "grade = sorted(df.grade.unique().tolist())\n",
    "sns.countplot(x='grade', data=df, hue='loan_status', order=grade)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sub_grade = sorted(df.sub_grade.unique().tolist())\n",
    "g = sns.countplot(x='sub_grade', data=df, hue='loan_status', order=sub_grade)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grade of majority of people those who have fully paid the loan is 'B' and have subgrade 'B3'.\n",
    "\n",
    "So from that we can infer that people with grade 'B' and subgrade 'B3' are more likely to fully pay the loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Name the top 2 afforded job titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "order = ['< 1 year', '1 year', '2 years', '3 years', '4 years', '5 years', \n",
    "          '6 years', '7 years', '8 years', '9 years', '10+ years',]\n",
    "g=sns.countplot(x='emp_length',data=df,hue='loan_status',order=order)\n",
    "g.set_xticklabels(g.get_xticklabels(),rotation=90)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.barh(df.emp_title.value_counts()[:30].index,df.emp_title.value_counts()[:30])\n",
    "plt.title(\"The most 30 jobs who took a loan\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manager and Teacher are the most afforded loan on titles\n",
    "\n",
    "Person who employed for more than 10 years has successfully paid of the loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(number):\n",
    "    if number == 0.0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def f2(number):\n",
    "    if number == 0.0:\n",
    "        return 0\n",
    "    elif number >= 1.0:\n",
    "        return 1\n",
    "    else:\n",
    "        return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pub_rec']=df.pub_rec.apply(f1)\n",
    "df['mort_acc']=df.mort_acc.apply(f2)\n",
    "df['pub_rec_bankruptcies']=df.pub_rec_bankruptcies.apply(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,30))\n",
    "\n",
    "plt.subplot(6,2,1)\n",
    "sns.countplot(x='pub_rec',data=df,hue='loan_status')\n",
    "\n",
    "plt.subplot(6,2,2)\n",
    "sns.countplot(x='initial_list_status',data=df,hue='loan_status')\n",
    "\n",
    "plt.subplot(6,2,3)\n",
    "sns.countplot(x='application_type',data=df,hue='loan_status')\n",
    "\n",
    "plt.subplot(6,2,4)\n",
    "sns.countplot(x='mort_acc',data=df,hue='loan_status')\n",
    "\n",
    "plt.subplot(6,2,5)\n",
    "sns.countplot(x='pub_rec_bankruptcies',data=df,hue='loan_status')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most the loan disbursed to the people whose do not hold bankrupties record have successfully paid loan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print(i, '-->> ', df[i].unique(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting term values to numerical val\n",
    "term_values={' 36 months': 36, ' 60 months':60}\n",
    "df['term'] = df.term.map(term_values)\n",
    "\n",
    "# Mapping the target variable\n",
    "df['loan_status'] = df.loan_status.map({'Fully Paid':0, 'Charged Off':1})\n",
    "\n",
    "# Initial List Status\n",
    "df['initial_list_status'].unique()\n",
    "np.array(['w', 'f'], dtype=object)\n",
    "list_status = {'w': 0, 'f': 1}\n",
    "df['initial_list_status'] = df.initial_list_status.map(list_status)\n",
    "\n",
    "# Let's fetch ZIP from address and then drop the remaining details -\n",
    "df['zip_code'] = df.address.apply(lambda x: x[-5:]) \n",
    "df['zip_code'].value_counts(normalize=True)*100\n",
    "\n",
    "for i in df.columns:\n",
    "    print(i, '-->> ', df[i].unique(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['issue_d'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['issue_month'] = df['issue_d'].apply(lambda x : str(x).split('-')[0])\n",
    "df['issue_year'] = df['issue_d'].apply(lambda x : str(x).split('-')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['issue_d'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['earliest_cr_line']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['earliest_cr_line_month'] = df['earliest_cr_line'].apply(lambda x : str(x).split('-')[0])\n",
    "df['earliest_cr_line_year'] = df['earliest_cr_line'].apply(lambda x : str(x).split('-')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['earliest_cr_line'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['address','zip_code','title',], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "  print(col, '->', df[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['term'] = label_encoder.fit_transform(df['term'])\n",
    "df['grade'] = label_encoder.fit_transform(df['grade'])\n",
    "df['sub_grade'] = label_encoder.fit_transform(df['sub_grade'])\n",
    "df['emp_length'] = label_encoder.fit_transform(df['emp_length'])\n",
    "df['home_ownership'] = label_encoder.fit_transform(df['home_ownership'])\n",
    "df['verification_status'] = label_encoder.fit_transform(df['verification_status'])\n",
    "df['loan_status'] = label_encoder.fit_transform(df['loan_status'])\n",
    "df['purpose'] = label_encoder.fit_transform(df['purpose'])\n",
    "df['pub_rec'] = label_encoder.fit_transform(df['pub_rec'])\n",
    "df['initial_list_status'] = label_encoder.fit_transform(df['initial_list_status'])\n",
    "df['application_type'] = label_encoder.fit_transform(df['application_type'])\n",
    "df['mort_acc'] = label_encoder.fit_transform(df['mort_acc'])\n",
    "df['pub_rec_bankruptcies'] = label_encoder.fit_transform(df['pub_rec_bankruptcies'])\n",
    "df['open_acc'] = label_encoder.fit_transform(df['open_acc'])\n",
    "df['issue_month'] = label_encoder.fit_transform(df['issue_month'])\n",
    "df['issue_year'] = label_encoder.fit_transform(df['issue_year'])\n",
    "df['earliest_cr_line_month'] = label_encoder.fit_transform(df['earliest_cr_line_month'])\n",
    "df['earliest_cr_line_year'] = label_encoder.fit_transform(df['earliest_cr_line_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'emp_title' : df['emp_title'], 'target' : df['loan_status']})\n",
    "target_mean = df1.groupby(by=['emp_title'])['target'].mean()\n",
    "df['emp_title'] = df1['emp_title'].map(target_mean)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('loan_status',axis=1)\n",
    "y=df['loan_status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,stratify=y,random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MinMaxScaler -\n",
    "For each value in a feature, MinMaxScaler subtracts the minimum value in the feature and then divides by the range. The range is the difference between the original maximum and original minimum.\n",
    "\n",
    "MinMaxScaler preserves the shape of the original distribution. It doesn’t meaningfully change the information embedded in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of Logistic Regression Classifier on test set: 0.888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sm = sm.add_constant(X_train)\n",
    "sm_model = sm.OLS(y_train, x_sm)\n",
    "result = sm_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "for coef, col in zip(model.coef_[0], df.columns):\n",
    "  dic[col] = abs(coef)\n",
    "a = sorted(dic.items(), key = lambda x: (x[1], x[0]))\n",
    "for i in a:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is significant value for false negative and false positive. Which will hamper our prediction due to type-1 or type-2 error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision score and recall score for full paid status is almost same indicates that model is doing decent job which correctly classified the both of the scenarios\n",
    "\n",
    "Precision score for charged off status is more than recall score which is perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Curve -\n",
    "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n",
    "\n",
    "True Positive Rate\n",
    "False Positive Rate\n",
    "True Positive Rate (TPR) is a synonym for recall and is therefore defined as follows:\n",
    "\n",
    "TPR=(TP)/(TP+FN)\n",
    "False Positive Rate (FPR) is defined as follows:\n",
    "\n",
    "FPR=(FP)/(FP+TN)\n",
    "An ROC curve plots TPR vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives. The following figure shows a typical ROC curve.\n",
    "\n",
    "AUC (Area under the ROC Curve) -\n",
    "AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1).\n",
    "\n",
    "AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. For example, given the following examples, which are arranged from left to right in ascending order of logistic regression predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = (model.predict_proba(X_test))[:,1]\n",
    "fpr, tpr, thr = roc_curve(y_test, prob)\n",
    "logit_roc_auc = roc_auc_score(y_test,model.predict(X_test))\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot(fpr,fpr,'--',color='red' )\n",
    "plt.plot(fpr,tpr,label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC-AUC curve is grossing the area which indicates that model is performing well.\n",
    "There is still room for some model improvement\n",
    "\n",
    "By collecting more data, using a more complex model, or tuning the hyperparameters, it is possible to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test,prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precission_recall_curve_plot(y_test,pred_proba_c1):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test,pred_proba_c1)\n",
    "    \n",
    "    threshold_boundary = thresholds.shape[0]\n",
    "    #plot precision\n",
    "    plt.plot(thresholds,precisions[0:threshold_boundary],linestyle='--',label='precision')\n",
    "    #plot recall\n",
    "    plt.plot(thresholds,recalls[0:threshold_boundary],label='recalls')\n",
    "    \n",
    "    start,end=plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start,end,0.1),2))\n",
    "    \n",
    "    plt.xlabel('Threshold Value')\n",
    "    plt.ylabel('Precision and Recall Value')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "precission_recall_curve_plot(y_test,model.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity check using Variance Inflation Factor (VIF) -\n",
    "Multicollinearity occurs when two or more independent variables are highly correlated with one another in a regression model. Multicollinearity can be a problem in a regression model because we would not be able to distinguish between the individual effects of the independent variables on the dependent variable.\n",
    "\n",
    "Multicollinearity can be detected via various methods. One such method is Variance Inflation Factor aka VIF. In VIF method, we pick each independent feature and regress it against all of the other independent features. VIF score of an independent variable represents how well the variable is explained by other independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vif(X):\n",
    "    # Calculating the VIF\n",
    "    vif=pd.DataFrame()\n",
    "    vif['Feature']=X.columns\n",
    "    vif['VIF']=[variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\n",
    "    vif['VIF']=round(vif['VIF'],2)\n",
    "    vif=vif.sort_values(by='VIF',ascending=False)\n",
    "    return vif\n",
    "\n",
    "calc_vif(X)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['int_rate'],axis=1,inplace=True)\n",
    "calc_vif(X)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['application_type'],axis=1,inplace=True)\n",
    "calc_vif(X)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['sub_grade'],axis=1,inplace=True)\n",
    "calc_vif(X)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['earliest_cr_line_year'],axis=1,inplace=True)\n",
    "calc_vif(X)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=['total_acc'],axis=1,inplace=True)\n",
    "calc_vif(X)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=scaler.fit_transform(X)\n",
    "\n",
    "kfold=KFold(n_splits=5)\n",
    "accuracy=np.mean(cross_val_score(model,X,y,cv=kfold,scoring='accuracy',n_jobs=-1))\n",
    "print(\"Cross Validation accuracy : {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm=SMOTE(random_state=42)\n",
    "X_train_res,y_train_res=sm.fit_resample(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = LogisticRegression(max_iter=1000)\n",
    "lr1.fit(X_train_res, y_train_res)\n",
    "predictions = lr1.predict(X_test)\n",
    "  \n",
    "# Classification Report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precission_recall_curve_plot(y_test, lr1.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('logistic_regression.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What percentage of customers have fully paid their Loan Amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df ['loan_status'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Comment about the correlation between Loan Amount and Installment features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spearman correlation coefficient between Loan Amount and Installmen is very high (i.e. 0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The majority of people have home ownership as _______."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df ['home_ownership'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['home_ownership'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mortgage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. People with grades ‘A’ are more likely to fully pay their loan. (T/F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True.\n",
    "\n",
    "Out of all people with grade 'A', 93% got their loan approved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Name the top 2 afforded job titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teacher & Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Thinking from a bank's perspective, which metric should our primary focus be on..\n",
    "ROC AUC\n",
    "Precision\n",
    "Recall\n",
    "F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be on f1 score.  as we need to give importance to both precision and recall. We don't want to miss potential customers and at the same time we also don't want to give loan to defaulters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. How does the gap in precision and recall affect the bank?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall score: 0.94 and Precision score: 0.85. Which tells us that there are more false positives than the false negatives.\n",
    "\n",
    "From Confusion Matrix it can be seen that FP = 10% of total cases & FN = 0.9% of Total Cases\n",
    "\n",
    "If Recall value is low (i.e. FN are high), it means Bank is loosing in opportunity cost.\n",
    "\n",
    "If Precision value is low (i.e. FP are high), it means Bank's NPA (defaulters) may increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Which were the features that heavily affected the outcome?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RFE we were able to identify top_20 features which has high impact on Outcome. This include:\n",
    "\n",
    "int_rate: Interest Rate\n",
    "\n",
    "sub_grade: loan subgrade\n",
    "\n",
    "term : number of payments on the loan\n",
    "\n",
    "home_ownership\n",
    "\n",
    "purpose\n",
    "\n",
    "application_type\n",
    "\n",
    "pincode (from address)\n",
    "\n",
    "emp_title: job title supplied by the Borrower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Will the results be affected by geographical location? (Yes/No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pincode (derived from address) has significant impact on the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we make sure that our model can detect real defaulters and there are less false positives? This is important as we can lose out on an opportunity to finance more individuals and earn interest on it.\n",
    "\n",
    "Answer - Since data is imbalances by making the data balance we can try to avoid false positives. For evaluation metrics, we should be focusing on the macro average f1-score because we don't want to make false positive prediction and at the same we want to detect the defualers.\n",
    "Since NPA (non-performing asset) is a real problem in this industry, it’s important we play safe and shouldn’t disburse loans to anyone\n",
    "\n",
    "Answer - Below are the most features and their importance while making the prediction. So these variables can help the managers to identify which are customers who are more likely to pay the loan amount fully,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actional Insights and Recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80% of the customers have paid the loan fully.\n",
    "\n",
    "20% of the customers are the defaulters.\n",
    "\n",
    "The organization can the trained model to make prediction for whether a person will likely to pay the loan amount or he will be a defaulter.\n",
    "\n",
    "Cross Validation accuracy and testing accuracy is almost same which infers model is performing the decent job. We can trust this model for unseen data\n",
    "\n",
    "By collecting more data, using a more complex model, or tuning the hyperparameters, it is possible to improve the model's performance.\n",
    "\n",
    "ROC AUC curve area of 0.73, the model is correctly classifying about 73% of the instances. This is a good performance, but there is still room for improvement.\n",
    "\n",
    "The precision-recall curve allows us to see how the precision and recall trade-off as we vary the threshold. A higher threshold will result in higher precision, but lower recall, and vice versa. The ideal point on the curve is the one that best meets the needs of the specific application.\n",
    "\n",
    "After balancing the dataset, there is significant change observed in the precion and recall score for both of the classes.\n",
    "\n",
    "Accuracy of Logistic Regression Classifier on test set: 0.888 which is decent and not by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
